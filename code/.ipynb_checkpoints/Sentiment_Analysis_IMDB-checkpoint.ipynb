{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5a33877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import en_core_web_sm\n",
    "\n",
    "en_core_web_sm.load()\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4b7e9",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ba87ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data file path\n",
    "PATH = \"E:\\\\NLP\\\\Final\\\\aclImdb\"\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf67214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating train files to positive and negative\n",
    "posFiles = [x for x in os.listdir(PATH + \"/train/pos/\") if x.endswith(\".txt\")]\n",
    "negFiles = [x for x in os.listdir(PATH + \"/train/neg/\") if x.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d04a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating test files to positive and negative\n",
    "test_pos_Files = [x for x in os.listdir(PATH + \"/test/pos/\") if x.endswith(\".txt\")]\n",
    "test_neg_Files = [x for x in os.listdir(PATH + \"/test/neg/\") if x.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa55841",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train = []\n",
    "N_train = []\n",
    "\n",
    "for nfile in negFiles:\n",
    "    with open(PATH + \"/train/neg/\" + nfile, encoding=\"utf-8\") as f:\n",
    "        N_train.append(f.read())\n",
    "        \n",
    "for pfile in posFiles:\n",
    "    with open(PATH + \"/train/pos/\" + pfile, encoding=\"utf-8\") as f:\n",
    "        P_train.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82fc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test = []\n",
    "N_test = []\n",
    "\n",
    "for ptestfile in test_pos_Files:\n",
    "    with open(PATH + \"/test/pos/\" + ptestfile, encoding=\"utf-8\") as f:\n",
    "        P_test.append(f.read())\n",
    "        \n",
    "for ntestfile in test_neg_Files:\n",
    "    with open(PATH + \"/test/neg/\" + ntestfile, encoding=\"utf-8\") as f:\n",
    "        N_test.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8fa410",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = pd.concat([\n",
    "    pd.DataFrame({\"review\": P_train, \"Label\": 1, \"file\": posFiles}),\n",
    "    pd.DataFrame({\"review\": N_train, \"Label\": -1, \"file\": negFiles})\n",
    "], ignore_index=True).sample(frac=1, random_state=1)\n",
    "\n",
    "reviews_test = pd.concat([\n",
    "    pd.DataFrame({\"review\": P_test, \"Label\": 1, \"file\": test_pos_Files}),\n",
    "    pd.DataFrame({\"review\": N_test, \"Label\": -1, \"file\": test_neg_Files})\n",
    "], ignore_index=True).sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ce3a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>I have copy of this on VHS, I think they (The ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6844_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488</th>\n",
       "      <td>After several extremely well ratings to the po...</td>\n",
       "      <td>1</td>\n",
       "      <td>7290_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>I still don't know why I forced myself to sit ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2740_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>Mt little sister and I are self-proclaimed hor...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10094_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>I have personally seen many Disney movies in m...</td>\n",
       "      <td>1</td>\n",
       "      <td>6150_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>Diane Keaton gave an outstanding performance i...</td>\n",
       "      <td>1</td>\n",
       "      <td>8610_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>This has to be creepiest, most twisted holiday...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3060_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>Do not expect a depiction of the \"truth\". Howe...</td>\n",
       "      <td>1</td>\n",
       "      <td>3423_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>The League of Gentlemen is one of the funniest...</td>\n",
       "      <td>1</td>\n",
       "      <td>9706_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Narratives  whether written, visual or poetic...</td>\n",
       "      <td>1</td>\n",
       "      <td>10211_7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  Label         file\n",
       "21492  I have copy of this on VHS, I think they (The ...     -1   6844_1.txt\n",
       "9488   After several extremely well ratings to the po...      1  7290_10.txt\n",
       "16933  I still don't know why I forced myself to sit ...     -1   2740_1.txt\n",
       "12604  Mt little sister and I are self-proclaimed hor...     -1  10094_1.txt\n",
       "8222   I have personally seen many Disney movies in m...      1   6150_7.txt\n",
       "...                                                  ...    ...          ...\n",
       "10955  Diane Keaton gave an outstanding performance i...      1  8610_10.txt\n",
       "17289  This has to be creepiest, most twisted holiday...     -1   3060_1.txt\n",
       "5192   Do not expect a depiction of the \"truth\". Howe...      1   3423_7.txt\n",
       "12172  The League of Gentlemen is one of the funniest...      1  9706_10.txt\n",
       "235    Narratives  whether written, visual or poetic...      1  10211_7.txt\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2be63e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>A movie theater with a bad history of past gru...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6844_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488</th>\n",
       "      <td>\"Here On Earth\" is a surprising beautiful roma...</td>\n",
       "      <td>1</td>\n",
       "      <td>7290_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>I just watched Descent. Gawds what an awful mo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2740_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>In a nutshell the movie is about a gang war in...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10094_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>Instead of watching the recycled history of \"P...</td>\n",
       "      <td>1</td>\n",
       "      <td>6150_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>This movie is a fascinating drama about the Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>8610_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>It's too kind to call this a \"fictionalized\" a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3060_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>I was unsure of this movie before renting and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3423_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>Just got out of an advance screening, and wow ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9706_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>I doubt if the real story of the development o...</td>\n",
       "      <td>1</td>\n",
       "      <td>10211_8.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  Label         file\n",
       "21492  A movie theater with a bad history of past gru...     -1   6844_2.txt\n",
       "9488   \"Here On Earth\" is a surprising beautiful roma...      1  7290_10.txt\n",
       "16933  I just watched Descent. Gawds what an awful mo...     -1   2740_3.txt\n",
       "12604  In a nutshell the movie is about a gang war in...     -1  10094_4.txt\n",
       "8222   Instead of watching the recycled history of \"P...      1   6150_7.txt\n",
       "...                                                  ...    ...          ...\n",
       "10955  This movie is a fascinating drama about the Ma...      1   8610_8.txt\n",
       "17289  It's too kind to call this a \"fictionalized\" a...     -1   3060_3.txt\n",
       "5192   I was unsure of this movie before renting and ...      1   3423_9.txt\n",
       "12172  Just got out of an advance screening, and wow ...      1   9706_7.txt\n",
       "235    I doubt if the real story of the development o...      1  10211_8.txt\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0deb5d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83007ca",
   "metadata": {},
   "source": [
    "Here we remove html tags, urls, special characters,Lemmanatize-\n",
    "which is better than stemming as it gives a proper word after cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40654740",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc823c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmvhtmltags(text):\n",
    "    remreg = re.compile('<.*?>')\n",
    "    cleartext = re.sub(remreg, '', text)\n",
    "    return cleartext\n",
    "\n",
    "def remove_urls(vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return (vTEXT)\n",
    "\n",
    "def rmvspclcharacter(text):\n",
    "    clearspcl = re.sub(r'[^A-Za-z0-9\\s.]', r'', str(text).lower())\n",
    "    clearspcl = re.sub(r'\\n', r' ', text)\n",
    "    clearspcl = \" \".join([word for word in text.split() if word not in stopWords])\n",
    "    return clearspcl\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, 'v') for word in text.split()]\n",
    "    return ('  '.join(lemmatized_words))\n",
    "\n",
    "\n",
    "# A function dataprocessing is defined where all other functions are included\n",
    "def dataprocessing(x):\n",
    "    x = rmvhtmltags(x)\n",
    "    x = remove_urls(x)\n",
    "    x = x.lower()\n",
    "    x = rmvspclcharacter(x)\n",
    "    x = remove_stopwords(x)\n",
    "    x = strip_punctuation(x)\n",
    "    x = strip_multiple_whitespaces(x)\n",
    "    x = lemmatize_words(x)\n",
    "\n",
    "    x = ' '.join([re.sub(r'\\d+', '', i) for i in word_tokenize(x)])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb05353",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train['review'] = reviews_train['review'].map(lambda x: dataprocessing(x))\n",
    "reviews_test['review'] = reviews_test['review'].map(lambda x: dataprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81491497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>copy vhs think the television network play yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6844_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488</th>\n",
       "      <td>extremely rat point superb extremely please fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>7290_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>know force sit thing film worth memorex dvd r ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2740_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>mt little sister self proclaim horror movie bu...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10094_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>personally see disney movies lifetime absolute...</td>\n",
       "      <td>1</td>\n",
       "      <td>6150_7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  Label         file\n",
       "21492  copy vhs think the television network play yea...     -1   6844_1.txt\n",
       "9488   extremely rat point superb extremely please fi...      1  7290_10.txt\n",
       "16933  know force sit thing film worth memorex dvd r ...     -1   2740_1.txt\n",
       "12604  mt little sister self proclaim horror movie bu...     -1  10094_1.txt\n",
       "8222   personally see disney movies lifetime absolute...      1   6150_7.txt"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30604a5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef885d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating them into lists\n",
    "y_train_label = reviews_train['Label'].tolist()\n",
    "x_train_review = reviews_train['review'].tolist()\n",
    "\n",
    "y_test_label = reviews_test['Label'].tolist()\n",
    "x_test_review = reviews_test['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d920960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_review, y_train_label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595ce45",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7486ad6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation for Logistic Regression on TF-IDF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.87418581, 0.88033602, 0.87793588])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "model_lr = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                  ('clf', LogisticRegression()), ])\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Cross Validation for Logistic Regression on TF-IDF\")\n",
    "cross_val_score(model_lr, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a26b15",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20bfb6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation for Support Vector Machine on TF-IDF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.87829962, 0.87827876, 0.88033602])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine \n",
    "model_svm_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()), ])\n",
    "model_svm_tfidf = model_svm_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Cross Validation for Support Vector Machine on TF-IDF\")\n",
    "cross_val_score(model_svm_tfidf, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd1914",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a66f9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation for Naive Bayes on Bag of Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8416181 , 0.85376307, 0.85222013])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes with Bag of Words\n",
    "model_nb_bow = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model_nb_bow = model_nb_bow.fit(X_train, y_train)\n",
    "\n",
    "print(\"Cross Validation for Naive Bayes on Bag of Words\")\n",
    "cross_val_score(model_nb_bow, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa3a46",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11fa995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e08006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1402078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform length\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "embedding_dim = 100\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model_lstm.fit(X_train_pad, encoded_y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, encoded_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model_lstm.evaluate(X_test_pad, encoded_y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "962fc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(training_model):\n",
    "    if training_model == 'LR':\n",
    "        print(\"Training Logistic regression model using TF-IDF\")\n",
    "        lrbow = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LogisticRegression()), ])\n",
    "        return lrbow\n",
    "\n",
    "    elif training_model == 'SVM':\n",
    "        print(\"Training SVM model using TF-IDF\")\n",
    "        svmbow = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC()), ])\n",
    "        return svmbow\n",
    "\n",
    "    elif training_model == 'NB':\n",
    "        print(\"Training NB model using BOW\")\n",
    "        nbbow = Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB()), ])\n",
    "        return nbbow\n",
    "    \n",
    "def model_fitting_mix():\n",
    "    training_model = ['LR', 'SVM', 'NB']\n",
    "    for i in training_model:\n",
    "        model_mix = model_training(i).fit(X_train, y_train)\n",
    "        predicted_mix = model_mix.predict(X_test)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy_mix = accuracy_score(y_test, predicted_mix)\n",
    "\n",
    "        # Precision, Recall, and F1 Score\n",
    "        precision_mix = precision_score(y_test, predicted_mix, average='weighted')\n",
    "        recall_mix = recall_score(y_test, predicted_mix, average='weighted')\n",
    "        f1_mix = f1_score(y_test, predicted_mix, average='weighted')\n",
    "\n",
    "        # Cross-validation metrics\n",
    "        scores_mix = cross_val_score(model_mix, X_train, y_train, cv=3)\n",
    "        print(f\"Metrics for model '{i}':\")\n",
    "        print(\"Accuracy on testing dataset:\", accuracy_mix)\n",
    "        print(\"Precision on testing dataset:\", precision_mix)\n",
    "        print(\"Recall on testing dataset:\", recall_mix)\n",
    "        print(\"F1 Score on testing dataset:\", f1_mix)\n",
    "        print(\"Mean Accuracy on training dataset (cross-validation):\", scores_mix.mean())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4c0ae765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic regression model using TF-IDF\n",
      "Metrics for model 'LR':\n",
      "Accuracy on testing dataset: 0.8854666666666666\n",
      "Precision on testing dataset: 0.8855887389603948\n",
      "Recall on testing dataset: 0.8854666666666666\n",
      "F1 Score on testing dataset: 0.8854483014528143\n",
      "Mean Accuracy on training dataset (cross-validation): 0.8774859028626015\n",
      "\n",
      "\n",
      "Training SVM model using TF-IDF\n",
      "Metrics for model 'SVM':\n",
      "Accuracy on testing dataset: 0.8790666666666667\n",
      "Precision on testing dataset: 0.8790691201803766\n",
      "Recall on testing dataset: 0.8790666666666667\n",
      "F1 Score on testing dataset: 0.8790643767903418\n",
      "Mean Accuracy on training dataset (cross-validation): 0.8789714669625178\n",
      "\n",
      "\n",
      "Training NB model using BOW\n",
      "Metrics for model 'NB':\n",
      "Accuracy on testing dataset: 0.8558666666666667\n",
      "Precision on testing dataset: 0.8566749090581058\n",
      "Recall on testing dataset: 0.8558666666666667\n",
      "F1 Score on testing dataset: 0.8558147891030761\n",
      "Mean Accuracy on training dataset (cross-validation): 0.8492004332761421\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fitting_mix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e28245",
   "metadata": {},
   "source": [
    "# Model Opitimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1364cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 5000, 10000],  # Adjust the number of features\n",
    "    'clf__C': [0.1, 1, 10],  # Regularization parameter for model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55117076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning using grid search for LR model\n",
      "Best Parameters:  {'clf__C': 1, 'tfidf__max_features': 10000}\n",
      "Mean Accuracy on training dataset (cross-validation):  0.8770858897995112\n",
      "Accuracy on testing dataset: 0.8838666666666667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with TfidfVectorizer\n",
    "model_lr_tuned = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search_lr = GridSearchCV(model_lr_tuned, param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy\n",
    "print(\"Hyperparameter tuning using grid search for LR model\")\n",
    "print(\"Best Parameters: \", grid_search_lr.best_params_)\n",
    "print(\"Mean Accuracy on training dataset (cross-validation): \", grid_search_lr.best_score_)\n",
    "print(\"Accuracy on testing dataset:\", grid_search_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39e3625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning using grid search for SVM model\n",
      "Best Parameters (SVM):  {'clf__C': 0.1, 'tfidf__max_features': 10000}\n",
      "Mean Accuracy on training dataset (cross-validation):  0.8779430620483275\n",
      "Accuracy on testing dataset:  0.8841333333333333\n"
     ]
    }
   ],
   "source": [
    "# SVM with TfidfVectorizer\n",
    "model_svm_tuned = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search_svm = GridSearchCV(model_svm_tuned, param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy\n",
    "print(\"Hyperparameter tuning using grid search for SVM model\")\n",
    "print(\"Best Parameters (SVM): \", grid_search_svm.best_params_)\n",
    "print(\"Mean Accuracy on training dataset (cross-validation): \", grid_search_svm.best_score_)\n",
    "print(\"Accuracy on testing dataset: \", grid_search_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "691e2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning using grid search for NB  model\n",
      "Best Parameters (Naive Bayes):  {'clf__alpha': 1.0, 'tfidf__max_features': 10000}\n",
      "Mean Accuracy on training dataset (cross-validation):  0.8527430618837655\n",
      "Accuracy on testing dataset:  0.8538666666666667\n"
     ]
    }
   ],
   "source": [
    "param_grid_nb = {\n",
    "    'tfidf__max_features': [1000, 5000, 10000],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Naive Bayes with Bag of Words\n",
    "model_nb_tuned = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search_nb = GridSearchCV(model_nb_tuned, param_grid_nb, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy\n",
    "print(\"Hyperparameter tuning using grid search for NB  model\")\n",
    "print(\"Best Parameters (Naive Bayes): \", grid_search_nb.best_params_)\n",
    "print(\"Mean Accuracy on training dataset (cross-validation): \", grid_search_nb.best_score_)\n",
    "print(\"Accuracy on testing dataset: \", grid_search_nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fa059",
   "metadata": {},
   "source": [
    "# Prediction (Deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "91bef546",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['fan season skip second  episodes lousy season lukewarm season  be okay i m point can millions people wrong probably not throw there season  far  episodes for me be criminally overrate slog protagonist care unknown soap opera esque hangups character develop element derivative hang hello john carpenter element tribal spiritualism inklings supernatural elements weigh possibly keep series afloat talk side mouth trappings suitable tie loose end run time space goofy time the scenes cgi laughably bad   bore time but end episodes bring element episode reveal more episodes approach feel letdown horizon kind mess line scatterbrained script get hard watch unfold slow motion say halfway point believe episode   wait happen temper expectations lot legitimate nationally publicated reviewers normally trust cuckoo cocoapuffs reviewable season preview get onboard subjective be and call horror theme season fall completely flat disturb scenes element reveal'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7548/2623695230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mTempering\u001b[0m \u001b[0mexpectations\u001b[0m \u001b[0mbecause\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlegitimate\u001b[0m \u001b[0mnationally\u001b[0m \u001b[0mpublicated\u001b[0m \u001b[0mreviewers\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mI\u001b[0m \u001b[0mnormally\u001b[0m \u001b[0mtrust\u001b[0m \u001b[0mseem\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mcuckoo\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcocoapuffs\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mreviewable\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mseason\u001b[0m \u001b[0mpreview\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mgot\u001b[0m \u001b[0mme\u001b[0m \u001b[0mback\u001b[0m \u001b[0monboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0msubjective\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;32mand\u001b[0m \u001b[0meven\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[1;34m\"horror\"\u001b[0m \u001b[0mthemed\u001b[0m \u001b[0mseason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mfalling\u001b[0m \u001b[0mcompletely\u001b[0m \u001b[0mflat\u001b[0m \u001b[0mother\u001b[0m \u001b[0mthan\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfew\u001b[0m \u001b[0mdisturbing\u001b[0m \u001b[0mscenes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melement\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwhat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0myet\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mrevealed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mpredicted_sentiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_word2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_review\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted Sentiment:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_sentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7548/2623695230.py\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[1;34m(model, review, threshold_low, threshold_high)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Use the trained model to predict sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mconfidence_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocessed_review\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Extract probabilities for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1668\u001b[0m         )\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    770\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['fan season skip second  episodes lousy season lukewarm season  be okay i m point can millions people wrong probably not throw there season  far  episodes for me be criminally overrate slog protagonist care unknown soap opera esque hangups character develop element derivative hang hello john carpenter element tribal spiritualism inklings supernatural elements weigh possibly keep series afloat talk side mouth trappings suitable tie loose end run time space goofy time the scenes cgi laughably bad   bore time but end episodes bring element episode reveal more episodes approach feel letdown horizon kind mess line scatterbrained script get hard watch unfold slow motion say halfway point believe episode   wait happen temper expectations lot legitimate nationally publicated reviewers normally trust cuckoo cocoapuffs reviewable season preview get onboard subjective be and call horror theme season fall completely flat disturb scenes element reveal'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, review, threshold_low=0.4, threshold_high=0.7):\n",
    "    # Preprocess user input\n",
    "    preprocessed_review = dataprocessing(user_review)\n",
    "    \n",
    "    # Use the trained model to predict sentiment\n",
    "    confidence_scores = model.predict_proba([preprocessed_review])[0]\n",
    "    \n",
    "    # Extract probabilities for each class\n",
    "    probability_negative = confidence_scores[0]\n",
    "    probability_positive = confidence_scores[1]\n",
    "\n",
    "    # Determine sentiment based on confidence scores and thresholds\n",
    "    if probability_positive >= threshold_high:\n",
    "        sentiment = \"Positive\"\n",
    "    elif probability_positive <= threshold_low:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    return sentiment, confidence_scores\n",
    "\n",
    "# Example usage\n",
    "user_review = \"\"\"\n",
    "Being a fan of the first season, skipping the second after 2 episodes in (lousy season), and lukewarm on season 3 (was okay), I'm at the point where \"can millions of people be wrong?\"\n",
    "\n",
    "Probably not, but just throwing that out there.\n",
    "\n",
    "Season 4, so far, through 3 episodes---for me---is a criminally overrated slog. Who is the protagonist? Why should we care about all the unknown \"soap opera-esque\" hangups between the characters as they develop? There is the element of being derivative hanging over this (hello, John Carpenter), but the element of tribal spiritualism and some inklings of supernatural elements weigh it down as much as they are possibly keeping this series afloat. Talking put of both sides of my mouth, but the trappings of a less than suitable tying up of loose ends is running out of time and space.\n",
    "\n",
    "It's goofy at times (the few scenes with CGI are laughably bad in 2023-2024), it's boring most of the time...but the ending of the episodes bring in the element of the next episode will reveal more, but after three episodes of that same approach, I feel a further letdown on the horizon.\n",
    "\n",
    "It's kind of a mess, bottom line, and the scatterbrained script is really getting hard to watch as it unfolds in slow motion.\n",
    "\n",
    "With that said, this is the halfway point I do believe. Episode 3 out of 6 and just waiting for something to happen.\n",
    "\n",
    "Tempering expectations because a lot of the legitimate nationally publicated reviewers that I normally trust seem to go cuckoo for cocoapuffs over their reviewable full season preview that got me back onboard, but subjective as it can be---and even some calling this more of a \"horror\" themed season, it's falling completely flat other than a few disturbing scenes and the element of what is yet to be revealed.\n",
    "\"\"\"\n",
    "predicted_sentiment, confidence = predict_sentiment(model_word2vec, user_review)\n",
    "\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)\n",
    "print(\"Confidence Scores:\", confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050954d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
